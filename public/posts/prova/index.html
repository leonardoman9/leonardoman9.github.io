<!doctype html>
<html lang="en">
  <head>
    

<script async src="https://www.googletagmanager.com/gtag/js?id=G-PW6SB9H75R"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PW6SB9H75R');
</script>

<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<title>TinyML: AI at the Edge |Leonardo Mannini</title>


<script>
if (navigator.serviceWorker) {
  navigator.serviceWorker.register("/sw.js", { scope: "/" });
}
</script>


<meta property="og:url" content="https://leonardoman9.github.io/posts/prova/">
  <meta property="og:site_name" content="Leonardo Mannini">
  <meta property="og:title" content="TinyML: AI at the Edge">
  <meta property="og:description" content="TinyML: Bringing AI to the World’s Smallest Devices TinyML (Tiny Machine Learning) represents a groundbreaking field at the intersection of embedded systems and machine learning. It focuses on deploying ML algorithms on ultra-low-power microcontrollers and embedded devices, often operating with just milliwatts or microwatts of power.
Why TinyML Matters The ability to run ML models directly on edge devices without cloud connectivity offers several advantages:
Privacy: Data remains on the device Latency: Immediate responses without network delays Efficiency: Reduced power consumption Reliability: Functions without internet connectivity Cost: Eliminates cloud computing expenses Technical Challenges Implementing neural networks on devices with as little as 256KB of flash memory and 64KB of RAM presents significant challenges. Consider the memory requirements of a typical CNN:">
  <meta property="og:locale" content="it_it">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-04-14T18:28:00+01:00">
    <meta property="article:modified_time" content="2023-04-14T18:28:00+01:00">
    <meta property="article:tag" content="Tinyml">
    <meta property="article:tag" content="Ai">
    <meta property="article:tag" content="Edge-Computing">
    <meta property="article:tag" content="Embedded-Systems">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="TinyML: AI at the Edge">
  <meta name="twitter:description" content="TinyML: Bringing AI to the World’s Smallest Devices TinyML (Tiny Machine Learning) represents a groundbreaking field at the intersection of embedded systems and machine learning. It focuses on deploying ML algorithms on ultra-low-power microcontrollers and embedded devices, often operating with just milliwatts or microwatts of power.
Why TinyML Matters The ability to run ML models directly on edge devices without cloud connectivity offers several advantages:
Privacy: Data remains on the device Latency: Immediate responses without network delays Efficiency: Reduced power consumption Reliability: Functions without internet connectivity Cost: Eliminates cloud computing expenses Technical Challenges Implementing neural networks on devices with as little as 256KB of flash memory and 64KB of RAM presents significant challenges. Consider the memory requirements of a typical CNN:">


<link
  rel="preload"
  href="/feather-icons/feather-sprite.svg"
  as="image"
/>


<link
  rel="stylesheet"
  type="text/css"
  media="screen"
  href="/css/feather-icon.css"
/>
<link
  rel="stylesheet"
  type="text/css"
  media="screen"
  href="/css/footer.css"
/>
<link
  rel="stylesheet"
  type="text/css"
  media="screen"
  href="/css/resume.css"
/>
<link
  rel="stylesheet"
  type="text/css"
  media="screen"
  href="/css/site-header.css"
/>
<link
  rel="stylesheet"
  type="text/css"
  media="screen"
  href="/css/site.css"
/>
<link
  rel="stylesheet"
  type="text/css"
  media="screen"
  href="/picocss/pico.blue.css"
/>


<style>
  main.container {
    max-width: 100% !important;
    width: 100% !important;
    padding-left: 2rem !important;
    padding-right: 2rem !important;
  }
  
  article.resume {
    width: 100% !important;
    max-width: 100% !important;
  }
  
  .grid {
    display: block !important;
    grid-template-columns: 1fr !important;
    max-width: 100% !important;
    width: 100% !important;
  }
  
  .grid > * {
    width: 100% !important;
    max-width: 100% !important;
    grid-column: 1 / -1 !important;
  }
</style> 
  </head>

  <body>
    <header class="container">
  <nav id="site-nav">
    <ul>
      <li>
        <h1><a href="https://leonardoman9.github.io/">Leonardo Mannini</a></h1>
      </li>
    </ul>
    <ul>
      
        <li><a href="/posts">Blog</a></li>
      
        <li><a href="/resume/">Resume</a></li>
      
    </ul>
  </nav>

  
    <div id="site-header-social" align="center"><a href="https://github.com/leonardoman9" rel="me" title="GitHub">
          <svg class="feather">
  <use href="https://leonardoman9.github.io/feather-icons/feather-sprite.svg#github"></use>
</svg>

        </a><span>|</span><a href="https://www.linkedin.com/in/leonardomannini/" rel="me" title="LinkedIn">
          <svg class="feather">
  <use href="https://leonardoman9.github.io/feather-icons/feather-sprite.svg#linkedin"></use>
</svg>

        </a></div><p></p></header>

    
  <main class="container">
    <article>
      <header>
        <h1>TinyML: AI at the Edge</h1>
        <div>
          Posted on
          <time datetime="2023-14-14 18:28">
            Apr 14, 2023
          </time>
          
        </div>
      </header>

      <main><h1 id="tinyml-bringing-ai-to-the-worlds-smallest-devices">TinyML: Bringing AI to the World&rsquo;s Smallest Devices</h1>
<p>TinyML (Tiny Machine Learning) represents a groundbreaking field at the intersection of embedded systems and machine learning. It focuses on deploying ML algorithms on ultra-low-power microcontrollers and embedded devices, often operating with just milliwatts or microwatts of power.</p>
<h2 id="why-tinyml-matters">Why TinyML Matters</h2>
<p>The ability to run ML models directly on edge devices without cloud connectivity offers several advantages:</p>
<ul>
<li><strong>Privacy</strong>: Data remains on the device</li>
<li><strong>Latency</strong>: Immediate responses without network delays</li>
<li><strong>Efficiency</strong>: Reduced power consumption</li>
<li><strong>Reliability</strong>: Functions without internet connectivity</li>
<li><strong>Cost</strong>: Eliminates cloud computing expenses</li>
</ul>
<h2 id="technical-challenges">Technical Challenges</h2>
<p>Implementing neural networks on devices with as little as 256KB of flash memory and 64KB of RAM presents significant challenges. Consider the memory requirements of a typical CNN:</p>
<p>$$\text{Memory Footprint} = \sum_{l=1}^{L} (I_l + O_l + W_l + B_l)$$</p>
<p>Where:</p>
<ul>
<li>$I_l$ is the input tensor size for layer $l$</li>
<li>$O_l$ is the output tensor size for layer $l$</li>
<li>$W_l$ is the weights size for layer $l$</li>
<li>$B_l$ is the bias size for layer $l$</li>
</ul>
<p>For example, a simple convolutional layer with 16 3×3 filters processing a 28×28 image with 8-bit quantization requires:</p>
<p>$$\text{Memory} = (28 \times 28 \times 1) + (26 \times 26 \times 16) + (3 \times 3 \times 1 \times 16) + 16 = 12,272 \text{ bytes}$$</p>
<h2 id="model-optimization-techniques">Model Optimization Techniques</h2>
<p>TinyML relies on several optimization strategies:</p>
<ol>
<li><strong>Quantization</strong>: Reducing precision from float32 to int8 or even binary</li>
<li><strong>Pruning</strong>: Removing unnecessary connections</li>
<li><strong>Knowledge Distillation</strong>: Training smaller networks to mimic larger ones</li>
<li><strong>Neural Architecture Search</strong>: Automatically finding efficient architectures</li>
<li><strong>Compiler Optimizations</strong>: Generating optimized code for specific microcontrollers</li>
</ol>
<h2 id="tinyml-development-workflow">TinyML Development Workflow</h2>
<pre tabindex="0"><code class="language-mermaid" data-lang="mermaid">graph TD
    A[Dataset Collection] --&gt; B[Model Design]
    B --&gt; C[Training on Desktop/Cloud]
    C --&gt; D[Quantization &amp; Optimization]
    D --&gt; E[Conversion to C/C++]
    E --&gt; F[Deployment on MCU]
    F --&gt; G[Evaluation &amp; Monitoring]
    G --&gt;|Iterate| B
</code></pre><h2 id="code-example-audio-keyword-classification-with-tensorflow-lite">Code Example: Audio Keyword Classification with TensorFlow Lite</h2>
<p>Here&rsquo;s a simplified implementation of an audio keyword spotter using TensorFlow Lite for Microcontrollers:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;tensorflow/lite/micro/all_ops_resolver.h&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;tensorflow/lite/micro/micro_error_reporter.h&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;tensorflow/lite/micro/micro_interpreter.h&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;tensorflow/lite/schema/schema_generated.h&#34;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&#34;model.h&#34;</span><span style="color:#75715e">  </span><span style="color:#75715e">// The converted model in a C array
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">// Globals for TFLite Micro
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">constexpr</span> <span style="color:#66d9ef">int</span> kTensorArenaSize <span style="color:#f92672">=</span> <span style="color:#ae81ff">30</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">1024</span>;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">uint8_t</span> tensor_arena[kTensorArenaSize];
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">setup</span>() {
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Set up logging
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">static</span> tflite<span style="color:#f92672">::</span>MicroErrorReporter micro_error_reporter;
</span></span><span style="display:flex;"><span>  tflite<span style="color:#f92672">::</span>ErrorReporter<span style="color:#f92672">*</span> error_reporter <span style="color:#f92672">=</span> <span style="color:#f92672">&amp;</span>micro_error_reporter;
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Map the model into a usable data structure
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">const</span> tflite<span style="color:#f92672">::</span>Model<span style="color:#f92672">*</span> model <span style="color:#f92672">=</span> tflite<span style="color:#f92672">::</span>GetModel(g_model);
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Create an interpreter to run the model
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">static</span> tflite<span style="color:#f92672">::</span>AllOpsResolver resolver;
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">static</span> tflite<span style="color:#f92672">::</span>MicroInterpreter interpreter(
</span></span><span style="display:flex;"><span>      model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Allocate memory for the model&#39;s input/output tensors
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  interpreter.AllocateTensors();
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Get pointers to the model&#39;s input and output tensors
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  TfLiteTensor<span style="color:#f92672">*</span> input <span style="color:#f92672">=</span> interpreter.input(<span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>  TfLiteTensor<span style="color:#f92672">*</span> output <span style="color:#f92672">=</span> interpreter.output(<span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">loop</span>() {
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Capture audio and extract MFCC features
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#75715e">// ...
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Run inference
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  interpreter.Invoke();
</span></span><span style="display:flex;"><span>  
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// Process results
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#75715e">// ...
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}
</span></span></code></pre></div><h2 id="real-world-applications">Real-World Applications</h2>
<p>TinyML is already transforming various industries:</p>
<ol>
<li><strong>Predictive Maintenance</strong>: Detecting machine anomalies using vibration sensors</li>
<li><strong>Agriculture</strong>: Smart irrigation and crop monitoring with minimal infrastructure</li>
<li><strong>Healthcare</strong>: Continuous monitoring of vital signs with battery-operated wearables</li>
<li><strong>Smart Homes</strong>: Voice interfaces and presence detection with extreme privacy</li>
<li><strong>Environmental Monitoring</strong>: Wildlife tracking and conservation efforts in remote areas</li>
</ol>
<h2 id="future-directions">Future Directions</h2>
<p>The field of TinyML is rapidly evolving with innovations in:</p>
<ul>
<li><strong>Hardware acceleration</strong>: Specialized neural processing units for MCUs</li>
<li><strong>Training algorithms</strong>: On-device training and adaptation</li>
<li><strong>Federated learning</strong>: Collaborative model improvement without data sharing</li>
<li><strong>Neuromorphic computing</strong>: Brain-inspired architectures for extreme efficiency</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>TinyML represents a paradigm shift in how we deploy AI, moving from cloud-centric approaches to truly distributed intelligence at the edge. As devices become more capable while maintaining tiny power budgets, we&rsquo;ll see intelligence embedded in everyday objects around us, fundamentally changing how we interact with technology.</p>
<h2 id="further-resources">Further Resources</h2>
<ul>
<li><a href="https://www.tinyml.org/">TinyML Foundation</a></li>
<li><a href="https://www.tensorflow.org/lite/microcontrollers">TensorFlow Lite for Microcontrollers</a></li>
<li><a href="https://www.edgeimpulse.com/">Edge Impulse</a></li>
<li><a href="https://docs.arduino.cc/library-examples/machine-learning-examples/">Arduino Machine Learning</a></li>
<li><a href="https://www.edx.org/learn/machine-learning/harvard-university-tiny-machine-learning">Harvard&rsquo;s TinyML Course</a></li>
</ul>
</main>

      <footer>
        <section class="footer-explore">
  
    <small><strong>Tags</strong></small>
    <nav>
      <ul>
        
          <li>
            <small>
              <a href="/tags/tinyml/">Tinyml</a>
            </small>
          </li>
        
          <li>
            <small>
              <a href="/tags/ai/">Ai</a>
            </small>
          </li>
        
          <li>
            <small>
              <a href="/tags/edge-computing/">Edge-Computing</a>
            </small>
          </li>
        
          <li>
            <small>
              <a href="/tags/embedded-systems/">Embedded-Systems</a>
            </small>
          </li>
        
      </ul>
    </nav>
  
  
    <small><strong>Categories</strong></small>
    <nav>
      <ul>
        
          <li>
            <small>
              <a href="/categories/deep-learning/">Deep Learning</a>
            </small>
          </li>
        
      </ul>
    </nav>
  
</section>

      </footer>
    </article>
  </main>

    <footer class="container">
  <section class="footer-explore">
    <h4>Explore</h4>
    
      <h5>
        <a href="/tags/">Tags</a>
      </h5>
      <nav>
        <ul>
          
            <li>
              <small>
                <a href="https://leonardoman9.github.io/tags/ai/">Ai</a>
              </small>
            </li>
          
            <li>
              <small>
                <a href="https://leonardoman9.github.io/tags/edge-computing/">Edge-Computing</a>
              </small>
            </li>
          
            <li>
              <small>
                <a href="https://leonardoman9.github.io/tags/audio/">Audio</a>
              </small>
            </li>
          
            <li>
              <small>
                <a href="https://leonardoman9.github.io/tags/embedded-systems/">Embedded-Systems</a>
              </small>
            </li>
          
            <li>
              <small>
                <a href="https://leonardoman9.github.io/tags/research/">Research</a>
              </small>
            </li>
          
        </ul>
      </nav>
    
  </section>

  <section>
    <p>
      <small>
        © 2025 
      </small>
    </p>
  </section>
</footer> 
  </body>
</html>
